{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff \n",
    "from ordpy import complexity_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 14:25:06.179173: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-18 14:25:06.295280: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-18 14:25:06.298900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 14:25:08.865705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/marco/Documents/IC/.ic_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functions\n",
    "import load_hasc\n",
    "import export_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf_analysis(param, DATA_NAME, PLOT, values, labels):\n",
    "    \"\"\"\n",
    "    Given a dataset, with it's respective values and labels, creates a new representation for the data, using SAX transformation. After that,\n",
    "    groups close symbols in the series, according to a given parameter.\n",
    "\n",
    "    -----------\n",
    "    Parameters:\n",
    "    param (tuple): (number of bins, window_size)\n",
    "    DATA_NAME (string): name of the folder in which to put the metrics and figures for the dataset.\n",
    "    PLOT (bool): indicates if a histogram of the symbols distribution is to be plotted.\n",
    "    values (pd.Dataframe): values of the dataset.\n",
    "    labels (pd.Series): labels of the dataset.\n",
    "\n",
    "    -----------\n",
    "    Returns:\n",
    "    new_rep (pd.Dataframe): new representation for the data values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform SAX transformation\n",
    "    sax_values = functions.run_sax(values, n_bins=param[0])\n",
    "    sax_data = pd.DataFrame(sax_values.reshape(values.shape))\n",
    "\n",
    "    # Compute dictionary of symbols\n",
    "    symbols_dict = functions.compute_symbols_dictionary(np.unique(sax_values), window_size=param[1])\n",
    "\n",
    "    # Create new representation using sliding windows\n",
    "    new_rep = sax_data.apply(lambda row : functions.create_new_representation(row, window_size=param[1], dict=symbols_dict), axis=1)\n",
    "\n",
    "    # Calculate jensenshannon distance based on the new representation\n",
    "    pairwise_js = functions.calculate_js_distance(new_rep)\n",
    "    eq_class, diff_class = functions.get_js_by_class(pairwise_js, labels)\n",
    "    export_creation.save_js_metrics(eq_class, diff_class, DATA_NAME, param[1], param[0])\n",
    "\n",
    "    # Calculate entropy and statistical complexity of the data\n",
    "    comp_entrop = [complexity_entropy(new_rep[i]) for i in range(new_rep.shape[0])]\n",
    "    comp_entrop = pd.DataFrame(comp_entrop, columns=['entropy', 'statistical_complexity'])\n",
    "    export_creation.plot_entropy_sc(comp_entrop, labels, DATA_NAME, param[1], param[0])\n",
    "\n",
    "    return new_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(DATA_NAME):\n",
    "    \"\"\"\n",
    "    Load a dataset.\n",
    "    \"\"\"\n",
    "    data = arff.loadarff(f'../data/{DATA_NAME}.arff')\n",
    "    data = pd.DataFrame(data[0])\n",
    "    labels = data['target']\n",
    "    values = data.drop('target', axis=1)\n",
    "    return values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME1 = 'AbnormalHeartbeat/AbnormalHeartbeat_TRAIN'\n",
    "DATA_NAME2 = 'AbnormalHeartbeat/AbnormalHeartbeat_TEST'\n",
    "PLOT_NAME = 'AbnormalHeartbeat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'ArticularyWordRecognition/ArticularyWordRecognition_TRAIN'\n",
    "# DATA_NAME2 = 'ArticularyWordRecognition/ArticularyWordRecognition_TEST'\n",
    "# PLOT_NAME = 'ArticularyWordRecognition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'Car/Car_TEST'\n",
    "# DATA_NAME2 = 'Car/Car_TRAIN'\n",
    "# PLOT_NAME = 'Car'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'ChlorineConcentration/ChlorineConcentration_TRAIN'\n",
    "# DATA_NAME2 = 'ChlorineConcentration/ChlorineConcentration_TEST'\n",
    "# PLOT_NAME = 'ChlorineConcentration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'ACSF1/ACSF1_TEST'\n",
    "# DATA_NAME2= 'ACSF1/ACSF1_TRAIN'\n",
    "# PLOT_NAME = 'ACSF1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'SyntheticControl/SyntheticControl_TRAIN' \n",
    "# DATA_NAME2 = 'SyntheticControl/SyntheticControl_TEST' \n",
    "# PLOT_NAME = 'SyntheticControl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'TwoPatterns/TwoPatterns_TRAIN'\n",
    "# DATA_NAME2 = 'TwoPatterns/TwoPatterns_TEST'\n",
    "# PLOT_NAME = 'TwoPatterns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'BeetleFly/BeetleFly_TRAIN'\n",
    "# DATA_NAME2 = 'BeetleFly/BeetleFly_TEST'\n",
    "# PLOT_NAME = 'BeetleFly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME1 = 'BirdChicken/BirdChicken_TRAIN'\n",
    "# DATA_NAME2 = 'BirdChicken/BirdChicken_TEST'\n",
    "# PLOT_NAME = 'BirdChicken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_data(DATA_NAME1)\n",
    "\n",
    "if DATA_NAME2:\n",
    "    data1, labels1 = load_data(DATA_NAME2)\n",
    "    data = pd.concat([data, data1], ignore_index=True)\n",
    "    labels = pd.concat([labels, labels1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PLOT_NAME = 'HASC'\n",
    "\n",
    "SEQUENCE_LEN = 600\n",
    "OVERLAP = 0.2\n",
    "colnames = ['timestamp', 'x', 'y', 'z']\n",
    "data_group_name = \"person101\"\n",
    "directories = os.listdir('../data/HASC')\n",
    "\n",
    "dfs = []\n",
    "df_label = []\n",
    "\n",
    "for dir in directories:\n",
    "    files = os.listdir('../data/HASC/{}/{}'.format(dir, data_group_name))\n",
    "    samples = 0\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            samples += 1\n",
    "            series = pd.read_csv('../data/HASC/{}/{}/{}'.format(dir, data_group_name, file))\n",
    "            series.columns = colnames\n",
    "            series = series.drop(columns='timestamp').apply(lambda x:np.linalg.norm(x.values), axis=1)\n",
    "            dfs.append(series)\n",
    "    df_label += [dir] * samples\n",
    "\n",
    "df = pd.concat(dfs, axis=1).T\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_label = pd.Series(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 14:25:21.111443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 14:25:21.119421: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_hasc.get_samples_from_time_series(df, df_label, SEQUENCE_LEN, OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN\n",
    "\n",
    "na_free = data.dropna()\n",
    "only_na = data[~data.index.isin(na_free.index)]\n",
    "labels.drop(only_na.index, inplace=True)\n",
    "data = na_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.reset_index(drop=True, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "PARAM = [(3, 3), (3, 4), (3, 5), (4, 3), (4, 4), (4, 5), (5, 3), (5, 4), (5, 5)] # (n_bins_sax, window_size)\n",
    "param = (4, 4)\n",
    "PLOT_DIST = False\n",
    "PLOT_SERIES = False\n",
    "HASC = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Transformation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Series\n",
    "\n",
    "if PLOT_SERIES:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "\n",
    "    for label in np.unique(labels):\n",
    "        class_data = data[labels == label][:5]\n",
    "\n",
    "        for i in range(len(class_data)):\n",
    "            plt.plot(class_data.iloc[i])\n",
    "            plt.title(f'Series from {label}')\n",
    "            Path(f'../fig/whole_series/{PLOT_NAME}/{str(label)}').mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(f'../fig/whole_series/{PLOT_NAME}/{str(label)}/fig{str(i)}.png')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations \n",
    "\n",
    "for param in PARAM:\n",
    "    print()\n",
    "    print(f'n_bins: {param[0]}, window_size: {param[1]}')\n",
    "    transf_analysis(param, PLOT_NAME, PLOT_DIST, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SAX transformation\n",
    "\n",
    "sax_values = functions.run_sax(data, n_bins=param[0])\n",
    "symbols_dict = functions.compute_symbols_dictionary(np.unique(sax_values), window_size=param[1])\n",
    "sax_data = pd.DataFrame(sax_values.reshape(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new representation using sliding window\n",
    "\n",
    "X = sax_data.apply(lambda row : functions.create_new_representation(row, window_size=param[1], dict=symbols_dict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new representation to external file\n",
    "\n",
    "X.to_csv('./new_rep.csv', index=None)\n",
    "\n",
    "# Read new representation from external file\n",
    "\n",
    "# X = pd.read_csv('./new_rep.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting modified data into train, validation and test input\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting original unmodified data into train, validation and test input\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "data_train, data_val, label_train, label_val = train_test_split(data_train, label_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNeighbors model\n",
    "\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting labels for the test set\n",
    "\n",
    "y_pred = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for predicted labels\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same process for the original unmodified data\n",
    "\n",
    "neigh.fit(data_train, label_train)\n",
    "label_pred = neigh.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy score for the test set\n",
    "\n",
    "acc_score = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "original_acc_score = metrics.balanced_accuracy_score(label_test, label_pred)\n",
    "\n",
    "print('With the transformation:', acc_score)\n",
    "print('Without the transformation:', original_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier for multiple parameters combinations\n",
    "\n",
    "def test_knn(data, labels, window_size, n_bins):\n",
    "    sax_values = functions.run_sax(data, n_bins=param[0])\n",
    "    symbols_dict = functions.compute_symbols_dictionary(np.unique(sax_values), window_size=param[1])\n",
    "    sax_data = pd.DataFrame(sax_values.reshape(data.shape))\n",
    "    X = sax_data.apply(lambda row : functions.create_new_representation(row, window_size, dict=symbols_dict), axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    data_train, data_val, label_train, label_val = train_test_split(data_train, label_train, test_size=0.25, random_state=42)\n",
    "    neigh = KNeighborsClassifier()\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    neigh.fit(data_train, label_train)\n",
    "    label_pred = neigh.predict(data_test)\n",
    "    original_cm = metrics.confusion_matrix(label_test, label_pred)\n",
    "    acc_score = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "    original_acc_score = metrics.balanced_accuracy_score(label_test, label_pred)\n",
    "    export_creation.save_classifier_metrics(acc_score, original_acc_score, cm, original_cm, PLOT_NAME, window_size, \n",
    "                                            n_bins, y_train, label_train, algorithm='KNN')\n",
    "    return acc_score, original_acc_score\n",
    "\n",
    "acc_scores = []\n",
    "og_acc_scores = []\n",
    "\n",
    "for param in PARAM:\n",
    "    acc, og_acc = test_knn(data, labels, param[1], param[0])\n",
    "    acc_scores.append(acc)\n",
    "    og_acc_scores.append(og_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_creation.plot_accuracies(acc_scores, og_acc_scores, PLOT_NAME, algorithm='KNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
